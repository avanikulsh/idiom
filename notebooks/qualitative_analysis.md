# Qualitative Analysis: Cross-Lingual Idiom Matching Quality

## Summary of Match Types

Based on manual inspection of top-scoring matches, we identified **8 distinct categories** of match quality:

---

## ‚úÖ 1. Perfect Equivalents (True Positives)

**What works:** Shared proverbs or conceptually identical metaphors across languages.

### Examples:

**English ‚Üî Japanese:**
- "easier said than done" ‚Üî "Ë®Ä„ÅÜ„ÅØÊòì„ÅèË°å„ÅÜ„ÅØÈõ£„Åó" (0.7515)
  - **Verdict:** ‚úîÔ∏è Perfect - Same proverb structure and meaning

**English ‚Üî French:**
- "go bananas" ‚Üî "avoir la banane" (0.7075)
  - **Verdict:** ‚úîÔ∏è Excellent - Both use fruit metaphor for excitement

**English ‚Üî Finnish:**
- "like peas in a pod" ‚Üî "kuin kaksi marjaa" (0.6444)
  - **Verdict:** ‚úîÔ∏è Good - Both compare similarity to paired foods

**Why this works:** Embeddings correctly captured semantic similarity when:
- Languages share cultural proverbs
- Metaphorical mappings align (emotion ‚Üí fruit, similarity ‚Üí food pairs)
- Usage contexts overlap significantly

---

## ‚ö†Ô∏è 2. Antonymous Metaphors (False Positives)

**What fails:** Embeddings conflate opposite emotions because they share domain/intensity.

### Examples:

**English ‚Üî Japanese:**
- "make your blood boil" (anger) ‚Üî "Ë°Ä„ÅåÂºï„Åè" (your blood runs cold - fear/shock) (0.7524)
  - **Verdict:** ‚ùå Wrong - Opposite emotional valence
  - **Why:** Model sees "blood + emotion intensity" = similar

**English ‚Üî French:**
- "have a cow" (don't overreact) ‚Üî "avoir la chair de poule" (goosebumps - fear) (0.7606)
  - **Verdict:** ‚ùå Wrong - Different emotion class (annoyance vs fear)

**Why this fails:**
- Embeddings capture domain similarity (emotion, body metaphor)
- But lack nuance to distinguish antonyms or emotion polarity
- High cosine similarity despite opposite meanings

---

## ‚ùå 3. Lexical Overlap Without Semantic Equivalence

**What fails:** Model matches on shared words/body parts, not meaning.

### Examples:

**English ‚Üî Japanese (all ~0.70-0.78 similarity):**
- "in one ear and out the other" (forgetfulness) matched to:
  - "ËÄ≥„Å´Êåü„ÇÄ" (overhear)
  - "ËÄ≥„ÇíÂ°û„Åê" (cover your ears)
  - "ËÄ≥„ÇíÊæÑ„Åæ„Åô" (listen carefully)
  - "ËÄ≥„ÅåÈÅ†„ÅÑ" (hard of hearing)

  **Verdict:** ‚ùå All wrong - Just lexical matching on "ear" (ËÄ≥)

**English ‚Üî Japanese:**
- "put your heads together" ‚Üî "È†≠„ÅåÂõ∫„ÅÑ" (stubborn-headed) (0.7574)
  - **Verdict:** ‚ùå Wrong - Shared "head" but opposite concepts (collaboration vs stubbornness)

**Why this fails:**
- Embeddings heavily weight lexical overlap
- Idioms with same body part clustered together
- Metaphorical meaning ignored in favor of surface tokens

---

## ‚ö†Ô∏è 4. Sentiment Match, Metaphor Mismatch

**What's partial:** Emotion/concept correct, but metaphorical imagery differs.

### Examples:

**English ‚Üî Japanese:**
- "bite someone's head off" (snap angrily) ‚Üî "È†≠„Å´Êù•„Çã" (anger rises to head) (0.7395)
  - **Verdict:** ‚ö†Ô∏è Semantic match (anger) but different metaphors (violence vs location)

**English ‚Üî Japanese:**
- "do someone's head in" (annoy) ‚Üî "Â∞ª„ÇíÂè©„Åè" (kick butt - motivate) (0.7943)
  - **Verdict:** ‚ö†Ô∏è Both intense/forceful but different target emotions

**Why this is partial:**
- Good for cross-lingual sentiment analysis
- Not suitable for idiom-to-idiom translation
- Useful for paraphrase but not equivalence

---

## ‚ùå 5. Non-Idiomatic Literal Actions

**What fails:** Idiomatic expression matched to literal action phrase.

### Examples:

**English ‚Üî Japanese:**
- "stick your neck out" (take a risk) matched to:
  - "Â£∞„ÇíÁ´ã„Å¶„Çã" (make a sound) (0.7251)
  - "È¶ñ„ÇíÊåØ„Çã" (shake your head) (literal body movement)

  **Verdict:** ‚ùå Wrong - Risk-taking vs physical gestures

**English ‚Üî Finnish:**
- "shake a leg" (hurry up) ‚Üî "ÊêñÈ†≠ÊôÉËÖ¶" (shake head and brain - literally) (0.6672)
  - **Verdict:** ‚ùå Wrong - Idiomatic urgency vs literal movement

**Why this fails:**
- One language's idiom = other language's literal description
- Embeddings can't distinguish figurative vs compositional meaning
- Body part + action = spurious similarity

---

## ‚ö†Ô∏è 6. Contextual Match, Not Idiom Match

**What's misleading:** Usage contexts semantically similar, but idioms aren't equivalent.

### Examples:

**English ‚Üî Japanese:**
- "over your head" (too complex) ‚Üî "Ê∞ó„Çí‰Ωø„ÅÜ„Å™" (don't mind me)
  - **Similarity based on:** Conversational contexts where someone's being dismissive/talking past someone
  - **Verdict:** ‚ö†Ô∏è Context overlap ‚â† idiom equivalence

**Why this happens:**
- Our representation: `idiom + contexts` (3 sentences)
- Model weights context heavily
- Good for: Document similarity, scene understanding
- Bad for: Idiom dictionary lookup

**Implication:** Confirms our symmetric design works for usage-based semantics, but may overfit to context genre (movie subtitles vs formal BNC).

---

## ‚ö†Ô∏è 7. Partial Analogies / Action Overlap

**What's partial:** Physical action or scenario similar, but idiomatic meaning diverges.

### Examples:

**English ‚Üî French:**
- "jeter l'√©ponge" (throw in the towel - give up) ‚Üî "chuck it down" (throw in bin)
  - **Verdict:** ‚ö†Ô∏è Both involve throwing/discarding, but different idiom classes

**English ‚Üî French:**
- "say cheese" (smile for photo) ‚Üî "en faire tout un fromage" (make a big cheese out of it - exaggerate) (0.6957)
  - **Verdict:** ‚ö†Ô∏è Both mention cheese, but unrelated meanings

**Why this happens:**
- Embeddings cluster by action scripts (throwing, food mentions)
- Metaphorical target differs despite surface similarity

---

## ‚ùå 8. Complete Mismatches (Embedding Artifacts)

**What fails:** No semantic, metaphorical, or lexical justification.

### Examples:

**English ‚Üî Japanese:**
- "meet your maker" (die) ‚Üî "‰∏ÄÊâã" (one hand/move - from games)
  - **Verdict:** ‚ùå Completely unrelated

**English ‚Üî Finnish:**
- "right as rain" ‚Üî "sataa kuin Esterin perse" (raining like Esther's ass)
  - **Verdict:** ‚ö†Ô∏è Both weather-related but semantically distant

**Why this happens:**
- Statistical noise in high-dimensional space
- Low-frequency idioms with sparse contexts
- Genre/domain effects (formal BNC vs casual subtitles)

---

## Quantitative Breakdown (Manual Annotation Needed)

To properly evaluate, we would need:

1. **Gold standard annotations:** Bilingual speakers mark true equivalents
2. **Precision@K calculation:** What % of top-K matches are valid?
3. **Category distribution:** How many fall into each of the 8 categories?

### Estimated Distribution (from manual inspection of top 30 matches):

| Category | Count (est.) | % |
|----------|--------------|---|
| ‚úÖ Perfect equivalents | 4-6 | ~15-20% |
| ‚ö†Ô∏è Sentiment match only | 6-8 | ~20-25% |
| ‚ùå Lexical overlap | 5-7 | ~15-20% |
| ‚ùå Antonymous metaphor | 2-3 | ~5-10% |
| ‚ö†Ô∏è Contextual match | 4-6 | ~15-20% |
| ‚ö†Ô∏è Partial analogy | 2-4 | ~5-10% |
| ‚ùå Literal vs idiom | 2-3 | ~5-10% |
| ‚ùå Complete mismatch | 1-2 | ~5% |

**Takeaway:** Only ~15-20% are high-quality idiom equivalents. Remaining ~80% are semantically related but not substitutable for translation.

---

## Implications for Research

### What We Learned:

1. **Embeddings ARE capturing cross-lingual semantics**
   - Even "wrong" matches show systematic patterns (body parts, emotions, actions)
   - Not random noise

2. **Context-based representations conflate usage similarity with equivalence**
   - Good: Captures how idioms function in discourse
   - Bad: Can't distinguish metaphorical structure from situational overlap

3. **Lexical grounding is too strong**
   - Shared words (ear, head, blood) dominate similarity
   - Need metaphor-aware representations

4. **Language pairs differ in match quality**
   - Japanese > Finnish > French might reflect:
     - Dataset size (more chances for good matches)
     - Cultural distance (shared proverbs with English)
     - Context genre mismatch (BNC vs subtitles)

### Recommendations for Improvement:

1. **Metaphor-aware embeddings:** Pre-train on metaphor datasets (VUA, MOH-X)
2. **Idiom-specific encoders:** Separate idiom from context during encoding
3. **Contrastive learning:** Train with antonym pairs to distinguish opposition
4. **Cross-lingual grounding:** Use bilingual idiom dictionaries for supervision
5. **Evaluation with gold standard:** Create annotated test set for precision/recall

---

## Conclusion

Multilingual sentence transformers with context-based representations show **promising but noisy** cross-lingual idiom matching:

- ‚úÖ **Strengths:** Captures semantic domains, usage patterns, sentiment
- ‚ùå **Weaknesses:** Lexical bias, metaphor conflation, antonym confusion
- üìä **Estimated precision:** ~15-20% for true equivalents in top matches

This validates that embeddings capture *something* meaningful about cross-lingual idiom semantics, but are **not yet suitable** for building idiom translation dictionaries without human curation.
